services:
  inference:
    build:
      context: ./inference
      dockerfile: Dockerfile
    image: strix-inference:latest
    container_name: strix-inference
    privileged: true
    restart: unless-stopped
    # CRITICAL: Pass GPU Hardware
    devices:
      - "/dev/kfd:/dev/kfd"
      - "/dev/dri:/dev/dri"
    group_add:
      - "video"
      - "render"
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HSA_OVERRIDE_GFX_VERSION=11.5.1
      - HIP_VISIBLE_DEVICES=0
    ipc: host
    security_opt:
      - seccomp:unconfined
    command: vllm serve openai/gpt-oss-120b --host 0.0.0.0 --port 8000 --trust-remote-code
