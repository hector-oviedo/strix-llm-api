services:
  # ---------------------------------------------------------------------------
  # INFERENCE ENGINE (The Muscle)
  # ---------------------------------------------------------------------------
  inference:
    build:
      context: ./inference
      dockerfile: Dockerfile
    image: strix-inference:latest
    container_name: strix-inference
    privileged: true
    restart: unless-stopped
    devices:
      - "/dev/kfd:/dev/kfd"
      - "/dev/dri:/dev/dri"
    group_add:
      - "video"
      - "render"
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HSA_OVERRIDE_GFX_VERSION=11.5.1
      - HIP_VISIBLE_DEVICES=0
    ipc: host
    security_opt:
      - seccomp:unconfined
    command: vllm serve openai/gpt-oss-120b --host 0.0.0.0 --port 8000 --trust-remote-code

  # ---------------------------------------------------------------------------
  # API GATEWAY (The Brain)
  # ---------------------------------------------------------------------------
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    image: strix-backend:latest
    container_name: strix-backend
    restart: always
    ports:
      - "8080:8080"
    environment:
      - INFERENCE_URL=http://inference:8000/v1
    depends_on:
      - inference
