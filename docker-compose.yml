services:
  # ---------------------------------------------------------------------------
  # INFERENCE ENGINE (vLLM Pre-built for ROCm)
  # ---------------------------------------------------------------------------
  inference:
    build:
      context: ./inference
      dockerfile: Dockerfile
    image: strix-inference:latest
    container_name: strix-inference
    privileged: true
    restart: unless-stopped
    devices:
      - "/dev/kfd:/dev/kfd"
      - "/dev/dri:/dev/dri"
    group_add:
      - "${VIDEO_GID}"
      - "${RENDER_GID}"
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      # Strix Halo GFX1151
      - HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION:-11.5.1}
      - PYTORCH_ROCM_ARCH=gfx1151
      - HIP_VISIBLE_DEVICES=0
      # HF Token for gated models
      - HF_TOKEN=${HF_TOKEN}
      # Performance
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
    ipc: host
    security_opt:
      - seccomp:unconfined
    # Override in .env with INFERENCE_MODEL
    command: >
      python3 -m vllm.entrypoints.openai.api_server
      --model ${INFERENCE_MODEL:-openai/gpt-oss-120b}
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --dtype bfloat16
      --max-model-len 32768
      --max-num-seqs 1

  # ---------------------------------------------------------------------------
  # API GATEWAY (FastAPI)
  # ---------------------------------------------------------------------------
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    image: strix-backend:latest
    container_name: strix-backend
    restart: always
    ports:
      - "8080:8080"
    environment:
      - INFERENCE_URL=http://inference:8000/v1
      - UPSTREAM_KEY=EMPTY
    depends_on:
      - inference
